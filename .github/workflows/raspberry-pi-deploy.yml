name: Deploy to Raspberry Pi

on:
  workflow_run:
    workflows:
      - Backend CI
      - Infrastructure CI
    types:
      - completed
    branches:
      - main
      - develop
  # Push 이벤트 추가 - develop 브랜치에 코드 푸시 시 자동 배포
  push:
    branches:
      - develop
    paths:
      - 'api/**'
      - 'infrastructure/**'
      - 'docker-compose.prod.yml'
      - '.github/workflows/backend-ci.yml'
  # 수동 실행도 지원
  workflow_dispatch:
    inputs:
      deploy_specific_services:
        description: '특정 서비스만 배포 (쉼표로 구분)'
        required: false
        default: ''
        type: string
      force_restart:
        description: '전체 서비스 강제 재시작'
        required: false
        default: false
        type: boolean
      backup_before_deploy:
        description: '배포 전 데이터 백업'
        required: false
        default: true
        type: boolean
      skip_verification:
        description: '배포 후 검증 단계 건너뛰기'
        required: false
        default: false
        type: boolean

jobs:
  prepare_deployment:
    name: Prepare Deployment
    runs-on: ubuntu-latest
    # 수정: 수동 실행 시 항상 성공, 자동 실행 시 선행 워크플로우가 성공한 경우에만 실행
    if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}
    outputs:
      environment: ${{ steps.set_env.outputs.environment }}
      image_tag: ${{ steps.set_env.outputs.image_tag }}
      specific_services: ${{ steps.set_env.outputs.specific_services }}
      force_restart: ${{ steps.set_env.outputs.force_restart }}
      backup_before_deploy: ${{ steps.set_env.outputs.backup_before_deploy }}
      skip_verification: ${{ steps.set_env.outputs.skip_verification }}
      platform_arch: ${{ steps.set_env.outputs.platform_arch }}
      kong_image: ${{ steps.set_env.outputs.kong_image }}
    steps:
      - id: set_env
        run: |
          BRANCH_NAME="${{ github.event.workflow_run.head_branch || github.ref_name }}"
          
          if [[ "$BRANCH_NAME" == "main" ]]; then
            echo "environment=production" >> $GITHUB_OUTPUT
            echo "image_tag=latest" >> $GITHUB_OUTPUT
          else
            echo "environment=development" >> $GITHUB_OUTPUT
            echo "image_tag=develop" >> $GITHUB_OUTPUT
          fi
          
          # 라즈베리파이 ARM64 아키텍처 설정
          echo "platform_arch=linux/arm64" >> $GITHUB_OUTPUT
          
          # Kong ARM64 호환 이미지 설정 (Kong 3.0은 ARM64 지원 안함)
          echo "kong_image=kong/kong-gateway:3.4.2.0" >> $GITHUB_OUTPUT
          
          echo "배포 브랜치: $BRANCH_NAME"
          echo "이미지 태그: $(if [[ \"$BRANCH_NAME\" == \"main\" ]]; then echo \"latest\"; else echo \"develop\"; fi)"
          echo "플랫폼 아키텍처: linux/arm64"
          echo "Kong 이미지: kong/kong-gateway:3.4.2.0 (ARM64 호환)"
          
          # 수동 실행 시 입력 파라미터 처리
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "specific_services=${{ github.event.inputs.deploy_specific_services }}" >> $GITHUB_OUTPUT
            
            # 불리언 값을 문자열로 변환하여 저장
            if [[ "${{ github.event.inputs.force_restart }}" == "true" ]]; then
              echo "force_restart=true" >> $GITHUB_OUTPUT
            else
              echo "force_restart=false" >> $GITHUB_OUTPUT
            fi
            
            if [[ "${{ github.event.inputs.backup_before_deploy }}" == "true" ]]; then
              echo "backup_before_deploy=true" >> $GITHUB_OUTPUT
            else
              echo "backup_before_deploy=false" >> $GITHUB_OUTPUT
            fi
            
            if [[ "${{ github.event.inputs.skip_verification }}" == "true" ]]; then
              echo "skip_verification=true" >> $GITHUB_OUTPUT
            else
              echo "skip_verification=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "specific_services=" >> $GITHUB_OUTPUT
            echo "force_restart=false" >> $GITHUB_OUTPUT
            echo "backup_before_deploy=true" >> $GITHUB_OUTPUT
            echo "skip_verification=false" >> $GITHUB_OUTPUT
          fi
          
          # 실행 정보 출력
          echo "배포 환경: ${{ github.event.workflow_run.head_branch || github.ref_name }}"
          echo "실행 방식: ${{ github.event_name }}"
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "수동 배포 옵션:"
            echo " - 특정 서비스: ${{ github.event.inputs.deploy_specific_services || '없음 (전체 배포)' }}"
            echo " - 강제 재시작: ${{ github.event.inputs.force_restart }}"
            echo " - 데이터 백업: ${{ github.event.inputs.backup_before_deploy }}"
            echo " - 검증 건너뛰기: ${{ github.event.inputs.skip_verification }}"
          fi

  validate_images:
    name: Validate Docker Images
    needs: prepare_deployment
    runs-on: ubuntu-latest
    outputs:
      images_available: ${{ steps.check_images.outputs.images_available }}
      missing_images: ${{ steps.check_images.outputs.missing_images }}
      build_required: ${{ steps.check_build.outputs.build_required }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Check Docker Desktop Status
        run: |
          echo "=== Docker Desktop 상태 확인 ==="
          
          # Docker 데몬 상태 확인
          if docker info >/dev/null 2>&1; then
            echo "✅ Docker Desktop이 실행 중입니다."
          else
            echo "❌ Docker Desktop이 실행되지 않았습니다."
            echo "Docker Desktop을 시작하는 중..."
            
            # 운영체제별 Docker Desktop 시작 명령어
            if [[ "$OSTYPE" == "darwin"* ]]; then
              # macOS
              open -a Docker || echo "Docker Desktop 자동 시작 실패. 수동으로 시작해주세요."
            elif [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]]; then
              # Windows
              start "" "C:\Program Files\Docker\Docker\Docker Desktop.exe" || echo "Docker Desktop 자동 시작 실패. 수동으로 시작해주세요."
            else
              # Linux
              systemctl --user start docker-desktop || echo "Docker Desktop 자동 시작 실패. 수동으로 시작해주세요."
            fi
            
            # Docker 시작 대기
            echo "Docker Desktop 시작을 기다리는 중..."
            for i in {1..30}; do
              if docker info >/dev/null 2>&1; then
                echo "✅ Docker Desktop이 성공적으로 시작되었습니다."
                break
              fi
              echo "대기 중... ($i/30)"
              sleep 10
            done
            
            # 최종 확인
            if ! docker info >/dev/null 2>&1; then
              echo "❌ Docker Desktop 시작 실패. 수동으로 Docker Desktop을 시작한 후 다시 시도해주세요."
              exit 1
            fi
          fi
          
          # Docker Buildx 확인
          if docker buildx version >/dev/null 2>&1; then
            echo "✅ Docker Buildx 사용 가능"
          else
            echo "⚠️ Docker Buildx 사용 불가"
          fi
      
      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
      
      - name: Check Service Build Context
        id: check_build
        run: |
          echo "=== 서비스 빌드 컨텍스트 확인 ==="
          
          # 각 서비스의 Dockerfile 존재 여부 확인
          SERVICES=("auth-service" "session-service" "user-service" "feedback-service" "report-service" "realtime-service")
          BUILD_REQUIRED=false
          
          for service in "${SERVICES[@]}"; do
            DOCKERFILE_PATH="api/${service}/Dockerfile"
            if [ -f "$DOCKERFILE_PATH" ]; then
              echo "✅ $service: Dockerfile 존재 ($DOCKERFILE_PATH)"
            else
              echo "❌ $service: Dockerfile 없음 ($DOCKERFILE_PATH)"
              BUILD_REQUIRED=true
            fi
          done
          
          # 인프라스트럭처 디렉토리 확인
          if [ -d "infrastructure" ]; then
            echo "✅ infrastructure 디렉토리 존재"
          else
            echo "❌ infrastructure 디렉토리 없음"
          fi
          
          echo "build_required=$BUILD_REQUIRED" >> $GITHUB_OUTPUT
      
      - name: Check Docker Images Availability
        id: check_images
        run: |
          REPO_OWNER=$(echo "${{ github.repository_owner }}" | tr "[:upper:]" "[:lower:]")
          IMAGE_TAG="${{ needs.prepare_deployment.outputs.image_tag }}"
          
          # 확인할 서비스 목록
          SERVICES=("auth-service" "session-service" "user-service" "feedback-service" "report-service" "realtime-service")
          MISSING_IMAGES=()
          AVAILABLE_IMAGES=()
          
          echo "=== Docker 이미지 존재 여부 확인 ==="
          echo "Repository Owner: $REPO_OWNER"
          echo "Image Tag: $IMAGE_TAG"
          echo "Platform: ${{ needs.prepare_deployment.outputs.platform_arch }}"
          
          for service in "${SERVICES[@]}"; do
            IMAGE_NAME="ghcr.io/${REPO_OWNER}/haptitalk-${service}:${IMAGE_TAG}"
            echo "확인 중: $IMAGE_NAME"
            
            # 이미지 매니페스트 확인 (ARM64 지원 여부 포함)
            if docker manifest inspect "$IMAGE_NAME" >/dev/null 2>&1; then
              # ARM64 아키텍처 지원 여부 확인
              if docker manifest inspect "$IMAGE_NAME" | jq -r '.manifests[]?.platform.architecture' | grep -q "arm64"; then
                echo "✅ $service: ARM64 이미지 사용 가능"
                AVAILABLE_IMAGES+=("$service")
              else
                echo "⚠️ $service: 이미지는 존재하지만 ARM64 지원 안함"
                MISSING_IMAGES+=("$service")
              fi
            else
              echo "❌ $service: 이미지 없음"
              MISSING_IMAGES+=("$service")
            fi
          done
          
          # Kong 이미지 ARM64 호환성 확인
          KONG_IMAGE="${{ needs.prepare_deployment.outputs.kong_image }}"
          echo "Kong 이미지 확인: $KONG_IMAGE"
          if docker manifest inspect "$KONG_IMAGE" | jq -r '.manifests[]?.platform.architecture' | grep -q "arm64"; then
            echo "✅ Kong: ARM64 호환 이미지 사용 가능"
          else
            echo "❌ Kong: ARM64 호환 이미지 없음"
          fi
          
          # 결과 출력
          echo "사용 가능한 이미지: ${AVAILABLE_IMAGES[*]}"
          echo "누락된 이미지: ${MISSING_IMAGES[*]}"
          
          # GitHub Actions 출력 설정
          if [ ${#MISSING_IMAGES[@]} -eq 0 ]; then
            echo "images_available=true" >> $GITHUB_OUTPUT
            echo "missing_images=" >> $GITHUB_OUTPUT
          else
            echo "images_available=false" >> $GITHUB_OUTPUT
            echo "missing_images=${MISSING_IMAGES[*]}" >> $GITHUB_OUTPUT
          fi
      
      - name: Handle Missing Images
        if: steps.check_images.outputs.images_available == 'false'
        run: |
          echo "⚠️ 일부 이미지가 누락되었습니다: ${{ steps.check_images.outputs.missing_images }}"
          echo "누락된 이미지가 있어도 배포를 계속 진행합니다."
          echo "누락된 서비스는 기존 이미지를 사용하거나 건너뛸 수 있습니다."

  setup_ssh_connection:
    name: Setup SSH Connection
    needs: prepare_deployment
    runs-on: ubuntu-latest
    steps:
      - name: Install cloudflared and Set up SSH
        run: |
          # cloudflared 설치
          curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o cloudflared
          chmod +x cloudflared
          sudo mv cloudflared /usr/local/bin
          cloudflared version
          
          # SSH 디렉토리 확인 및 생성
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          
          # 시크릿에 저장된 SSH 키 사용
          echo "${{ secrets.RASPBERRY_PI_SSH_KEY }}" > ~/.ssh/id_ed25519
          chmod 600 ~/.ssh/id_ed25519
          
          # cloudflared로 로컬 포트 2222를 라즈베리파이 SSH 포트로 포워딩
          echo "Cloudflared TCP 포트 포워딩 설정 중..."
          nohup cloudflared access tcp --hostname pi.eumgyeol.com --url 127.0.0.1:2222 --loglevel debug > cloudflared.log 2>&1 &
          CLOUDFLARED_PID=$!
          echo "Cloudflared PID: $CLOUDFLARED_PID"
          
          # 설정 확인을 위해 대기
          sleep 10
          
          # cloudflared 상태 확인
          if ps -p $CLOUDFLARED_PID > /dev/null; then
            echo "cloudflared 프로세스가 실행 중입니다."
            
            # cloudflared 로그 확인
            echo "cloudflared 로그:"
            cat cloudflared.log || echo "로그 파일이 없습니다."
            
            # SSH 설정 - 로컬 포트로 연결
            {
              echo "Host raspberry-pi"
              echo "  HostName localhost"
              echo "  Port 2222"
              echo "  User ${{ secrets.RASPBERRY_PI_USER }}"
              echo "  IdentityFile ~/.ssh/id_ed25519"
              echo "  StrictHostKeyChecking no"
              echo "  UserKnownHostsFile /dev/null"
              echo "  LogLevel DEBUG3"
              echo "  ConnectTimeout 30"
              echo "  ServerAliveInterval 60"
              echo "  ServerAliveCountMax 10"
            } > ~/.ssh/config
            chmod 600 ~/.ssh/config
          else
            echo "cloudflared 프로세스 시작 실패"
            cat cloudflared.log || echo "로그 파일이 없습니다"
            
            # 재시도
            echo "cloudflared 재시작을 시도합니다..."
            pkill cloudflared || echo "기존 프로세스가 없습니다"
            sleep 2
            nohup cloudflared access tcp --hostname pi.eumgyeol.com --url 127.0.0.1:2222 --loglevel debug > cloudflared_retry.log 2>&1 &
            CLOUDFLARED_PID=$!
            sleep 10
            
            if ps -p $CLOUDFLARED_PID > /dev/null; then
              echo "cloudflared 재시작 성공"
              {
                echo "Host raspberry-pi"
                echo "  HostName localhost"
                echo "  Port 2222"
                echo "  User ${{ secrets.RASPBERRY_PI_USER }}"
                echo "  IdentityFile ~/.ssh/id_ed25519"
                echo "  StrictHostKeyChecking no"
                echo "  UserKnownHostsFile /dev/null"
                echo "  LogLevel DEBUG3"
                echo "  ConnectTimeout 30"
                echo "  ServerAliveInterval 60"
                echo "  ServerAliveCountMax 10"
              } > ~/.ssh/config
              chmod 600 ~/.ssh/config
            else
              echo "cloudflared 재시작 실패"
              cat cloudflared_retry.log || echo "재시도 로그 파일이 없습니다"
              exit 1
            fi
          fi
      
      - name: Test SSH Connection
        id: ssh_test
        run: |
          echo "SSH 연결 테스트 중..."
          
          # SSH 키 기반 인증으로 연결 시도
          if ssh -o ConnectTimeout=20 raspberry-pi 'echo "Connection successful" && uptime'; then
            echo "ssh_connected=true" >> $GITHUB_OUTPUT
          else
            echo "ssh_connected=false" >> $GITHUB_OUTPUT
            echo "연결 실패. 자세한 로그:"
            ssh -vvv raspberry-pi 'echo test' || true
            cat cloudflared.log || echo "cloudflared 로그 파일이 없습니다."
            
            # 재시도
            echo "SSH 연결 재시도..."
            pkill cloudflared || echo "cloudflared 프로세스가 없습니다"
            sleep 2
            
            # cloudflared 재설치
            curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o cloudflared
            chmod +x cloudflared
            sudo mv cloudflared /usr/local/bin
            
            nohup cloudflared access tcp --hostname pi.eumgyeol.com --url 127.0.0.1:2222 --loglevel debug > cloudflared_retry2.log 2>&1 &
            sleep 10
            
            if ssh -o ConnectTimeout=20 raspberry-pi 'echo "Connection successful on retry" && uptime'; then
              echo "ssh_connected=true" >> $GITHUB_OUTPUT
            else
              echo "SSH 재시도 실패. 워크플로우를 중단합니다."
              cat cloudflared_retry2.log || echo "재시도 로그 파일이 없습니다"
              exit 1
            fi
          fi

  check_raspberry_pi:
    name: Check Raspberry Pi Health
    needs: [prepare_deployment, setup_ssh_connection, validate_images]
    runs-on: ubuntu-latest
    steps:
      - name: Setup SSH Connection Again
        run: |
          # 이전 cloudflared 프로세스 종료
          pkill cloudflared || echo "실행 중인 cloudflared 프로세스가 없습니다"
          
          # cloudflared 설치
          echo "cloudflared 설치 중..."
          curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o cloudflared
          chmod +x cloudflared
          sudo mv cloudflared /usr/local/bin
          cloudflared version
          
          # SSH 디렉토리 확인 및 생성
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          
          # SSH 키 확인 (이미 있으면 다시 생성하지 않음)
          if [ ! -f ~/.ssh/id_ed25519 ]; then
            echo "${{ secrets.RASPBERRY_PI_SSH_KEY }}" > ~/.ssh/id_ed25519
            chmod 600 ~/.ssh/id_ed25519
          fi
          
          # 새 cloudflared TCP 터널 설정
          echo "Cloudflared TCP 포트 포워딩 설정 중..."
          nohup cloudflared access tcp --hostname pi.eumgyeol.com --url 127.0.0.1:2222 --loglevel debug > cloudflared_health.log 2>&1 &
          CLOUDFLARED_PID=$!
          echo "Cloudflared PID: $CLOUDFLARED_PID"
          
          # 설정 확인을 위해 대기
          sleep 10
          
          # cloudflared 상태 확인
          if ps -p $CLOUDFLARED_PID > /dev/null; then
            echo "cloudflared 프로세스가 실행 중입니다."
            
            # cloudflared 로그 확인
            echo "cloudflared 로그:"
            cat cloudflared_health.log || echo "로그 파일이 없습니다."
            
            # SSH 설정 - 로컬 포트로 연결
            {
              echo "Host raspberry-pi"
              echo "  HostName localhost"
              echo "  Port 2222"
              echo "  User ${{ secrets.RASPBERRY_PI_USER }}"
              echo "  IdentityFile ~/.ssh/id_ed25519"
              echo "  StrictHostKeyChecking no"
              echo "  UserKnownHostsFile /dev/null"
              echo "  LogLevel DEBUG3"
              echo "  ConnectTimeout 30"
              echo "  ServerAliveInterval 60"
              echo "  ServerAliveCountMax 10"
            } > ~/.ssh/config
            chmod 600 ~/.ssh/config
          else
            echo "cloudflared 프로세스 시작 실패"
            cat cloudflared_health.log || echo "로그 파일이 없습니다"
            exit 1
          fi
          
      - name: Test SSH Connection
        run: |
          echo "SSH 연결 테스트 중..."
          
          # SSH 키 기반 인증으로 연결 시도 
          if ssh -o ConnectTimeout=20 raspberry-pi 'echo "Connection successful" && uptime'; then
            echo "SSH 연결 성공"
          else
            echo "SSH 연결 실패. 자세한 로그:"
            ssh -vvv raspberry-pi 'echo test' || true
            cat cloudflared_health.log || echo "cloudflared 로그 파일이 없습니다."
            
            # 재시도
            echo "SSH 연결 재시도..."
            pkill cloudflared || echo "cloudflared 프로세스가 없습니다"
            sleep 2
            
            nohup cloudflared access tcp --hostname pi.eumgyeol.com --url 127.0.0.1:2222 --loglevel debug > cloudflared_retry_health.log 2>&1 &
            sleep 10
            
            if ssh -o ConnectTimeout=20 raspberry-pi 'echo "Connection successful on retry" && uptime'; then
              echo "SSH 재연결 성공"
            else
              echo "SSH 재시도 실패. 워크플로우를 중단합니다."
              cat cloudflared_retry_health.log || echo "재시도 로그 파일이 없습니다"
              exit 1
            fi
          fi
          
      - name: Setup Docker Authentication on Raspberry Pi
        run: |
          # GitHub Container Registry 인증 설정
          echo "GitHub Container Registry 인증 설정 중..."
          
          # read:packages 권한이 있는 PAT 토큰 사용
          echo "${{ secrets.GHCR_PAT }}" | ssh raspberry-pi "cat > ~/.github_token"
          
          # 라즈베리파이에서 Docker 로그인 실행
          ssh raspberry-pi '
            echo "GitHub Container Registry 인증 설정 중..."
            cat ~/.github_token | docker login ghcr.io -u ${{ github.repository_owner }} --password-stdin
            rm ~/.github_token  # 보안을 위해 토큰 파일 삭제
          '
      
      - name: Check Disk Space
        id: disk_check
        run: |
          # 디스크 공간 확인
          DISK_INFO=$(ssh raspberry-pi 'df -h | grep -E "/$"')
          
          # 디스크 사용량 추출 (% 제거)
          DISK_USAGE=$(echo "$DISK_INFO" | awk '{print $5}' | sed 's/%//')
          echo "disk_usage=$DISK_USAGE" >> $GITHUB_OUTPUT
          
          # 사용 가능한 공간 확인 (MB 단위)
          AVAILABLE_SPACE=$(ssh raspberry-pi 'df -m | grep -E "/$" | awk "{print \$4}"')
          echo "available_space=${AVAILABLE_SPACE}MB" >> $GITHUB_OUTPUT
          
          # 최소 필요 공간: 500MB
          if [ "$AVAILABLE_SPACE" -lt 500 ]; then
            echo "disk_warning=true" >> $GITHUB_OUTPUT
          else
            echo "disk_warning=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Check RAM
        id: ram_check
        run: |
          # 메모리 정보 확인
          RAM_INFO=$(ssh raspberry-pi 'free -m | grep "Mem:"')
          
          # 사용 가능한 메모리 추출 (MB)
          AVAILABLE_RAM=$(echo "$RAM_INFO" | awk '{print $7}')
          echo "available_ram=${AVAILABLE_RAM}MB" >> $GITHUB_OUTPUT
          
          # 최소 필요 메모리: 200MB
          if [ "$AVAILABLE_RAM" -lt 200 ]; then
            echo "ram_warning=true" >> $GITHUB_OUTPUT
          else
            echo "ram_warning=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Display Resource Info
        run: |
          echo "디스크 사용량: ${{ steps.disk_check.outputs.disk_usage }}%"
          echo "사용 가능한 디스크 공간: ${{ steps.disk_check.outputs.available_space }}"
          echo "사용 가능한 메모리: ${{ steps.ram_check.outputs.available_ram }}"
      
      - name: Clean Up if Necessary
        if: steps.disk_check.outputs.disk_warning == 'true'
        run: |
          echo "디스크 공간 부족, 정리 작업 수행 중..."
          ssh raspberry-pi '
            # 미사용 도커 리소스 정리
            docker system prune -af --volumes
            
            # 로그 파일 정리
            find /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk -name "*.log" -type f -exec rm -f {} \;
            
            # 임시 파일 삭제
            sudo find /tmp -type f -atime +5 -delete
          '
          
          # 정리 후 디스크 공간 다시 확인
          DISK_USAGE_AFTER=$(ssh raspberry-pi 'df -h | grep -E "/$" | awk "{print \$5}" | sed "s/%//"')
          
          echo "정리 후 디스크 사용량: ${DISK_USAGE_AFTER}%"

  backup_data:
    name: Backup Data
    needs: [prepare_deployment, setup_ssh_connection, check_raspberry_pi, validate_images]
    runs-on: ubuntu-latest
    if: needs.prepare_deployment.outputs.backup_before_deploy == 'true'
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup SSH Connection Again
        run: |
          # 이전 cloudflared 프로세스 종료
          pkill cloudflared || echo "실행 중인 cloudflared 프로세스가 없습니다"
          
          # cloudflared 설치
          echo "cloudflared 설치 중..."
          curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o cloudflared
          chmod +x cloudflared
          sudo mv cloudflared /usr/local/bin
          cloudflared version
          
          # SSH 디렉토리 확인 및 생성
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          
          # SSH 키 확인 (이미 있으면 다시 생성하지 않음)
          if [ ! -f ~/.ssh/id_ed25519 ]; then
            echo "${{ secrets.RASPBERRY_PI_SSH_KEY }}" > ~/.ssh/id_ed25519
            chmod 600 ~/.ssh/id_ed25519
          fi
          
          # 새 cloudflared TCP 터널 설정
          echo "Cloudflared TCP 포트 포워딩 설정 중..."
          nohup cloudflared access tcp --hostname pi.eumgyeol.com --url 127.0.0.1:2222 --loglevel debug > cloudflared_backup.log 2>&1 &
          CLOUDFLARED_PID=$!
          echo "Cloudflared PID: $CLOUDFLARED_PID"
          
          # 설정 확인을 위해 대기
          sleep 10
          
          # cloudflared 상태 확인
          if ps -p $CLOUDFLARED_PID > /dev/null; then
            echo "cloudflared 프로세스가 실행 중입니다."
            
            # cloudflared 로그 확인
            echo "cloudflared 로그:"
            cat cloudflared_backup.log || echo "로그 파일이 없습니다."
            
            # SSH 설정 - 로컬 포트로 연결
            {
              echo "Host raspberry-pi"
              echo "  HostName localhost"
              echo "  Port 2222"
              echo "  User ${{ secrets.RASPBERRY_PI_USER }}"
              echo "  IdentityFile ~/.ssh/id_ed25519"
              echo "  StrictHostKeyChecking no"
              echo "  UserKnownHostsFile /dev/null"
              echo "  LogLevel DEBUG3"
              echo "  ConnectTimeout 30"
              echo "  ServerAliveInterval 60"
              echo "  ServerAliveCountMax 10"
            } > ~/.ssh/config
            chmod 600 ~/.ssh/config
          else
            echo "cloudflared 프로세스 시작 실패"
            cat cloudflared_backup.log || echo "로그 파일이 없습니다"
            exit 1
          fi
          
      - name: Test SSH Connection
        run: |
          echo "SSH 연결 테스트 중..."
          
          # SSH 키 기반 인증으로 연결 시도 
          if ssh -o ConnectTimeout=20 raspberry-pi 'echo "Connection successful" && uptime'; then
            echo "SSH 연결 성공"
          else
            echo "SSH 연결 실패. 자세한 로그:"
            ssh -vvv raspberry-pi 'echo test' || true
            cat cloudflared_backup.log || echo "cloudflared 로그 파일이 없습니다."
            
            # 재시도
            echo "SSH 연결 재시도..."
            pkill cloudflared || echo "cloudflared 프로세스가 없습니다"
            sleep 2
            
            nohup cloudflared access tcp --hostname pi.eumgyeol.com --url 127.0.0.1:2222 --loglevel debug > cloudflared_retry_backup.log 2>&1 &
            sleep 10
            
            if ssh -o ConnectTimeout=20 raspberry-pi 'echo "Connection successful on retry" && uptime'; then
              echo "SSH 재연결 성공"
            else
              echo "SSH 재시도 실패. 워크플로우를 중단합니다."
              cat cloudflared_retry_backup.log || echo "재시도 로그 파일이 없습니다"
              exit 1
            fi
          fi
      
      - name: Setup Docker Authentication on Raspberry Pi
        run: |
          # GitHub Container Registry 인증 설정
          echo "GitHub Container Registry 인증 설정 중..."
          
          # read:packages 권한이 있는 PAT 토큰 사용
          echo "${{ secrets.GHCR_PAT }}" | ssh raspberry-pi "cat > ~/.github_token"
          
          # 라즈베리파이에서 Docker 로그인 실행
          ssh raspberry-pi '
            echo "GitHub Container Registry 인증 설정 중..."
            cat ~/.github_token | docker login ghcr.io -u ${{ github.repository_owner }} --password-stdin
            rm ~/.github_token  # 보안을 위해 토큰 파일 삭제
          '
      
      - name: Transfer Docker Compose Files
        run: |
          # 기존 docker-compose.yml 확인 및 수정
          echo "Docker Compose 파일 준비 중..."
          
          # 레포지토리에서 docker-compose.yml 파일 사용 (이미 체크아웃됨)
          if [ -f "docker-compose.yml" ]; then
            echo "기존 docker-compose.yml 파일을 사용합니다."
          else
            echo "docker-compose.yml 파일이 없습니다. 기본 파일을 생성합니다."
            # 기본 docker-compose.yml 생성 (간단한 버전)
            cat > docker-compose.yml << 'EOF'
          version: "3.8"
          services:
            postgres:
              image: postgres:14-alpine
              restart: always
              environment:
                POSTGRES_USER: ${POSTGRES_USER}
                POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
                POSTGRES_DB: ${POSTGRES_DB}
              ports:
                - "${POSTGRES_PORT:-5432}:5432"
            redis:
              image: redis:7-alpine
              restart: always
              command: redis-server --requirepass ${REDIS_PASSWORD}
              ports:
                - "${REDIS_PORT:-6379}:6379"
          volumes:
            postgres_data:
            redis_data:
          EOF
          fi
          
          # ARM64 호환 docker-compose.prod.yml 파일 생성 (별도 파일로)
          echo "ARM64 호환 docker-compose.prod.yml 생성 중..."
          
          # 라즈베리파이에 파일 전송
          scp -o StrictHostKeyChecking=no docker-compose.yml raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/
          
          # 기존 프로덕션 파일이 있으면 사용, 없으면 개발용을 복사
          if [ -f "docker-compose.prod.yml" ]; then
            echo "기존 docker-compose.prod.yml 파일을 사용합니다 (ARM64 호환 완료)."
            scp -o StrictHostKeyChecking=no docker-compose.prod.yml raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/
          else
            echo "docker-compose.prod.yml이 없어서 기본 파일을 복사합니다."
            cp docker-compose.yml docker-compose.prod.yml
            scp -o StrictHostKeyChecking=no docker-compose.prod.yml raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/
          fi
          
          scp -o StrictHostKeyChecking=no docker-compose.prod.yml raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/
          
          # 필요한 디렉토리 구조 생성
          ssh raspberry-pi << 'REMOTE_SCRIPT'
            mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/database/postgres/init
            mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/database/mongodb/init
            mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/database/redis/init
            mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/api-gateway
            mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/static-web/html
          REMOTE_SCRIPT
          
          # Kong 설정 파일 전송 (필요시)
          if [ -f "infrastructure/api-gateway/kong.yml" ]; then
            scp -o StrictHostKeyChecking=no infrastructure/api-gateway/kong.yml raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/api-gateway/
          else
            # 기본 Kong 설정 파일 생성
            mkdir -p infrastructure/api-gateway
            cat > infrastructure/api-gateway/kong.yml << 'EOF'
          _format_version: "2.1"
          _transform: true
          
          services:
            - name: default-service
              url: http://static-web
              routes:
                - name: default-route
                  paths:
                    - /
          EOF
            scp -o StrictHostKeyChecking=no infrastructure/api-gateway/kong.yml raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/api-gateway/
          fi
          
          # .env 파일을 단계별로 생성
          echo "POSTGRES_USER=${{ secrets.POSTGRES_USER }}" > .env
          echo "POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD }}" >> .env
          echo "POSTGRES_DB=${{ secrets.POSTGRES_DB }}" >> .env
          echo "POSTGRES_VOLUME_NAME=postgres_data" >> .env
          echo "POSTGRES_PORT=5432" >> .env
          echo "" >> .env
          echo "MONGO_USER=${{ secrets.MONGO_USER }}" >> .env
          echo "MONGO_PASSWORD=${{ secrets.MONGO_PASSWORD }}" >> .env
          echo "MONGO_DB=${{ secrets.MONGO_DB }}" >> .env
          echo "MONGODB_VOLUME_NAME=mongodb_data" >> .env
          echo "MONGO_PORT=27017" >> .env
          echo "" >> .env
          echo "REDIS_PASSWORD=${{ secrets.REDIS_PASSWORD }}" >> .env
          echo "REDIS_VOLUME_NAME=redis_data" >> .env
          echo "REDIS_PORT=6379" >> .env
          echo "" >> .env
          echo "ZOOKEEPER_PORT=2181" >> .env
          echo "KAFKA_PORT=9092" >> .env
          echo "KAFKA_UI_PORT=8080" >> .env
          echo "" >> .env
          echo "KONG_PROXY_PORT=8000" >> .env
          echo "KONG_HTTPS_PORT=8443" >> .env
          echo "KONG_ADMIN_PORT=8001" >> .env
          echo "KONG_VOLUME_NAME=kong_data" >> .env
          echo "" >> .env
          echo "JWT_ACCESS_SECRET=${{ secrets.JWT_ACCESS_SECRET }}" >> .env
          echo "JWT_REFRESH_SECRET=${{ secrets.JWT_REFRESH_SECRET }}" >> .env
          echo "JWT_SESSION_SECRET=${{ secrets.JWT_SESSION_SECRET }}" >> .env
          echo "JWT_ACCESS_EXPIRES_IN=15m" >> .env
          echo "JWT_REFRESH_EXPIRES_IN=7d" >> .env
          echo "JWT_SESSION_EXPIRES_IN=30d" >> .env
          echo "" >> .env
          echo "KAFKA_TOPIC_SESSION_EVENTS=session-events" >> .env
          echo "KAFKA_TOPIC_ANALYSIS_RESULTS=analysis-results" >> .env
          echo "KAFKA_TOPIC_FEEDBACK_COMMANDS=feedback-commands" >> .env
          echo "KAFKA_TOPIC_USER_ACTIVITY=user-activity" >> .env
          echo "" >> .env
          echo "AUTH_SERVICE_PORT=3000" >> .env
          echo "REALTIME_SERVICE_PORT=3001" >> .env
          echo "SESSION_SERVICE_PORT=3002" >> .env
          echo "FEEDBACK_SERVICE_PORT=3003" >> .env
          echo "USER_SERVICE_PORT=3004" >> .env
          echo "REPORT_SERVICE_PORT=3005" >> .env
          echo "" >> .env
          echo "LOG_LEVEL=info" >> .env
          echo "FRONTEND_URL=http://${{ secrets.RASPBERRY_PI_IP }}:8080" >> .env
          echo "EMAIL_FROM=no-reply@haptitalk.com" >> .env
          echo "" >> .env
          echo "DEPLOY_VERSION=$(date +%Y%m%d%H%M%S)" >> .env
          echo "DEPLOY_ENVIRONMENT=${{ needs.prepare_deployment.outputs.environment }}" >> .env
          echo "" >> .env
          echo "# 모니터링 시스템 설정" >> .env
          echo "ELASTICSEARCH_PORT=9200" >> .env
          echo "KIBANA_PORT=5601" >> .env
          echo "LOGSTASH_BEATS_PORT=5044" >> .env
          echo "LOGSTASH_TCP_PORT=5000" >> .env
          echo "LOGSTASH_API_PORT=9600" >> .env
          echo "JAEGER_UI_PORT=16686" >> .env
          echo "JAEGER_COLLECTOR_GRPC_PORT=14250" >> .env
          echo "JAEGER_COLLECTOR_HTTP_PORT=14268" >> .env
          echo "JAEGER_OTLP_PORT=4318" >> .env
          echo "JAEGER_OTLP_GRPC_PORT=4317" >> .env
          echo "OTEL_COLLECTOR_PORT=4317" >> .env
          echo "OTEL_COLLECTOR_HTTP_PORT=4318" >> .env
          echo "OTEL_COLLECTOR_PROM_PORT=8889" >> .env
          echo "OTEL_COLLECTOR_ZPAGES_PORT=55679" >> .env
          echo "PROMETHEUS_PORT=9090" >> .env
          echo "GRAFANA_PORT=3333" >> .env
          echo "NODE_EXPORTER_PORT=9100" >> .env
          echo "" >> .env
          echo "# Exporters 포트 설정" >> .env
          echo "MONGODB_EXPORTER_PORT=9216" >> .env
          echo "POSTGRES_EXPORTER_PORT=9187" >> .env
          echo "REDIS_EXPORTER_PORT=9121" >> .env
          echo "ELASTICSEARCH_EXPORTER_PORT=9114" >> .env
          echo "LOGSTASH_EXPORTER_PORT=9304" >> .env
          echo "KONG_BLACKBOX_EXPORTER_PORT=9701" >> .env
          echo "KIBANA_BLACKBOX_EXPORTER_PORT=9702" >> .env
          echo "MONGODB_BLACKBOX_EXPORTER_PORT=9703" >> .env
          echo "" >> .env
          echo "# ELK 스택 인증 설정" >> .env
          echo "ELASTIC_USERNAME=elastic" >> .env
          echo "ELASTIC_PASSWORD=changeme" >> .env
          echo "" >> .env
          echo "# Grafana 설정" >> .env
          echo "GRAFANA_ADMIN_USER=admin" >> .env
          echo "GRAFANA_ADMIN_PASSWORD=admin" >> .env
          echo "" >> .env
          echo "# 볼륨 설정" >> .env
          echo "ELASTICSEARCH_VOLUME_NAME=elasticsearch_data" >> .env
          echo "PROMETHEUS_VOLUME_NAME=prometheus_data" >> .env
          echo "GRAFANA_VOLUME_NAME=grafana_data" >> .env
          echo "JAEGER_VOLUME_NAME=jaeger_data" >> .env
          echo "LOGSTASH_DATA_VOLUME_NAME=logstash_data" >> .env
          
          scp -o StrictHostKeyChecking=no .env raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/
          
          # 인프라스트럭처 디렉토리 전송 (존재하는 경우에만)
          if [ -d "infrastructure" ]; then
            scp -o StrictHostKeyChecking=no -r infrastructure raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/
            # infrastructure/automation 폴더의 모든 .sh 파일에 실행 권한 부여
            ssh raspberry-pi "find /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure -name '*.sh' -type f -exec chmod +x {} \;"
          else
            echo "infrastructure 디렉토리가 존재하지 않습니다. 원격에서 생성합니다."
            # 원격에서 필요한 디렉토리 구조 생성
            ssh raspberry-pi << 'REMOTE_SCRIPT2'
              mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/database/postgres/init
              mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/database/mongodb/init
              mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/database/redis/init
              mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/api-gateway
              mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/static-web/html
              mkdir -p /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/infrastructure/automation
          REMOTE_SCRIPT2
          fi
          
          # 배포 스크립트 생성 및 전송
          cat > deploy.sh << 'DEPLOY_EOF'
          #!/bin/bash
          set -e
          
          # 배포 로그 시작
          DEPLOY_LOG="/home/${USER}/haptitalk/logs/deploy_$(date +%Y%m%d_%H%M%S).log"
          mkdir -p /home/${USER}/haptitalk/logs
          
          echo "===== 배포 시작: $(date) =====" | tee -a $DEPLOY_LOG
          cd /home/${USER}/haptitalk
          
          # 디렉토리 구조 확인
          mkdir -p infrastructure/database/postgres/init
          mkdir -p infrastructure/database/mongodb/init
          mkdir -p infrastructure/database/redis/init
          mkdir -p infrastructure/api-gateway
          mkdir -p infrastructure/messaging/kafka/init
          mkdir -p infrastructure/static-web/html
          mkdir -p infrastructure/monitoring
          
          # 환경 변수 검증
          echo "환경 변수 검증 중..." | tee -a $DEPLOY_LOG
          if [ -z "$POSTGRES_PASSWORD" ] || [ -z "$REDIS_PASSWORD" ] || [ -z "$JWT_ACCESS_SECRET" ]; then
            echo "오류: 필수 환경 변수가 설정되지 않았습니다." | tee -a $DEPLOY_LOG
            exit 1
          fi
          
          # Docker 네트워크 생성
          docker network create haptitalk_network 2>/dev/null || echo "네트워크가 이미 존재합니다."
          
          # 이미지 가져오기 (ARM64 플랫폼 지정)
          echo "ARM64 도커 이미지 업데이트 중..." | tee -a $DEPLOY_LOG
          docker-compose -f docker-compose.prod.yml pull --platform linux/arm64 || echo "일부 이미지를 가져오지 못했습니다. 계속 진행합니다." | tee -a $DEPLOY_LOG
          
          # 기존 컨테이너 상태 저장 (롤백용)
          echo "현재 컨테이너 상태 백업 중..." | tee -a $DEPLOY_LOG
          docker-compose -f docker-compose.prod.yml ps > /home/${USER}/haptitalk/container_state_before_deploy.txt || true
          
          # 특정 서비스만 재시작 여부 확인
          SPECIFIC_SERVICES="$1"
          FORCE_RESTART="$2"
          MISSING_IMAGES="$3"
          
          # 실제 배포 시작
          echo "배포 시작..." | tee -a $DEPLOY_LOG
          
          if [ "$FORCE_RESTART" == "true" ]; then
            echo "강제 재시작 모드: 모든 컨테이너를 중지하고 재시작합니다." | tee -a $DEPLOY_LOG
            docker-compose -f docker-compose.prod.yml down --remove-orphans
            docker-compose -f docker-compose.prod.yml up -d
          elif [ -n "$SPECIFIC_SERVICES" ]; then
            echo "특정 서비스만 재시작: $SPECIFIC_SERVICES" | tee -a $DEPLOY_LOG
            IFS=',' read -ra SERVICES <<< "$SPECIFIC_SERVICES"
            for service in "${SERVICES[@]}"; do
              echo "서비스 재시작: $service" | tee -a $DEPLOY_LOG
              # 서비스 존재 여부 확인 후 재시작
              if grep -q "$service:" docker-compose.prod.yml; then
                docker-compose -f docker-compose.prod.yml stop $service || true
                docker-compose -f docker-compose.prod.yml rm -f $service || true
                docker-compose -f docker-compose.prod.yml up -d $service
              else
                echo "경고: $service 서비스가 docker-compose.prod.yml에 정의되어 있지 않습니다." | tee -a $DEPLOY_LOG
              fi
            done
          else
            echo "인프라 서비스 배포 중..." | tee -a $DEPLOY_LOG
            # 기본 인프라 서비스만 시작
            docker-compose -f docker-compose.prod.yml up -d postgres mongodb redis zookeeper kafka kafka-ui kong static-web node-exporter kibana logstash mongodb-exporter postgres-exporter redis-exporter elasticsearch-exporter kong-blackbox-exporter kibana-blackbox-exporter mongodb-blackbox-exporter
            
            echo "애플리케이션 서비스 배포 중..." | tee -a $DEPLOY_LOG
            # 각 서비스 개별 시작 (오류 격리)
            for service in auth-service realtime-service session-service feedback-service user-service report-service; do
              if ! echo "$MISSING_IMAGES" | grep -q "$service"; then
                echo "서비스 시작: $service" | tee -a $DEPLOY_LOG
                docker-compose -f docker-compose.prod.yml up -d $service || echo "경고: $service 시작 실패" | tee -a $DEPLOY_LOG
              else
                echo "건너뛰기: $service (이미지 누락)" | tee -a $DEPLOY_LOG
              fi
            done
          fi
          
          # 배포 완료 확인
          echo "컨테이너 상태:" | tee -a $DEPLOY_LOG
          docker-compose -f docker-compose.prod.yml ps | tee -a $DEPLOY_LOG
          
          # 실행 중인 컨테이너 수 확인
          RUNNING_COUNT=$(docker-compose -f docker-compose.prod.yml ps --services --filter "status=running" | wc -l)
          echo "실행 중인 컨테이너 수: $RUNNING_COUNT" | tee -a $DEPLOY_LOG
          
          # 헬스 체크
          echo "헬스 체크 수행 중..." | tee -a $DEPLOY_LOG
          sleep 10
          
          # 주요 서비스 헬스 체크
          for service in postgres redis kong; do
            if docker-compose -f docker-compose.prod.yml ps $service | grep -q "Up"; then
              echo "$service: 정상 실행 중" | tee -a $DEPLOY_LOG
            else
              echo "$service: 실행 중이 아님" | tee -a $DEPLOY_LOG
            fi
          done
          
          echo "===== 배포 완료: $(date) =====" | tee -a $DEPLOY_LOG
          DEPLOY_EOF
          
          chmod +x deploy.sh
          scp -o StrictHostKeyChecking=no deploy.sh raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/
          
          # 롤백 스크립트 생성
          cat > rollback.sh << 'ROLLBACK_EOF'
          #!/bin/bash
          set -e
          
          echo "===== 롤백 시작: $(date) ====="
          cd /home/${USER}/haptitalk
          
          # 현재 상태 저장
          docker-compose -f docker-compose.prod.yml ps > container_state_before_rollback.txt || true
          
          # 컨테이너 중지
          echo "컨테이너 중지 중..."
          docker-compose -f docker-compose.prod.yml down --remove-orphans
          
          # 이전 이미지 태그로 롤백 (develop -> latest, latest -> previous)
          if [ -f ".env.backup" ]; then
            echo "이전 환경 설정으로 복원 중..."
            cp .env.backup .env
          fi
          
          # 서비스 재시작
          echo "서비스 재시작 중..."
          docker-compose -f docker-compose.prod.yml up -d
          
          echo "===== 롤백 완료: $(date) ====="
          docker-compose -f docker-compose.prod.yml ps
          ROLLBACK_EOF
          
          chmod +x rollback.sh
          scp -o StrictHostKeyChecking=no rollback.sh raspberry-pi:/home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk/

      - name: Deploy Services
        id: deploy
        run: |
          # 배포 실행
          SPECIFIC_SERVICES="${{ needs.prepare_deployment.outputs.specific_services }}"
          FORCE_RESTART="${{ needs.prepare_deployment.outputs.force_restart }}"
          IMAGES_AVAILABLE="${{ needs.validate_images.outputs.images_available }}"
          MISSING_IMAGES="${{ needs.validate_images.outputs.missing_images }}"
          BUILD_REQUIRED="${{ needs.validate_images.outputs.build_required }}"
          
          echo "배포 시작: 환경=${{ needs.prepare_deployment.outputs.environment }}"
          echo "특정 서비스: $SPECIFIC_SERVICES"
          echo "강제 재시작: $FORCE_RESTART"
          echo "이미지 사용 가능: $IMAGES_AVAILABLE"
          echo "누락된 이미지: $MISSING_IMAGES"
          echo "빌드 필요: $BUILD_REQUIRED"
          
          # 라즈베리파이에서 배포 실행
          ssh raspberry-pi "
            cd /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk
            
            # 환경 변수 백업 (롤백용)
            cp .env .env.backup || echo '기존 .env 파일이 없습니다.'
            
            # 환경 변수 파일에 GitHub Container Registry 설정 추가
            echo 'GITHUB_REPOSITORY_OWNER=$(echo "${{ github.repository_owner }}" | tr "[:upper:]" "[:lower:]")' >> .env
            echo 'IMAGE_TAG=${{ needs.prepare_deployment.outputs.image_tag }}' >> .env
            echo 'KONG_IMAGE=${{ needs.prepare_deployment.outputs.kong_image }}' >> .env
            echo 'DEPLOY_TIMESTAMP=$(date +%Y%m%d_%H%M%S)' >> .env
            echo 'DEPLOY_BRANCH=${{ github.ref_name }}' >> .env
            echo 'DEPLOY_COMMIT=${{ github.sha }}' >> .env
            
            # GitHub Container Registry 인증
            echo '${{ secrets.GHCR_PAT }}' | docker login ghcr.io -u ${{ github.repository_owner }} --password-stdin
            
            echo '=== ARM64 호환 배포 시작 ==='
            
            # 기존 컨테이너 정리
            echo '=== 기존 컨테이너 정리 중 ==='
            docker-compose -f docker-compose.prod.yml down --remove-orphans || echo '기존 컨테이너 정리 완료'
            
            # .env 파일 명시적 로드
            echo '=== 환경 변수 로드 ==='
            if [ -f \".env\" ]; then
              set -a  # 환경 변수 자동 export 활성화
              source .env
              set +a  # 환경 변수 자동 export 비활성화
              echo \".env 파일 로드 완료\"
            else
              echo \"⚠️ .env 파일이 없습니다!\"
            fi
            
            # 환경 변수 확인
            echo '=== 환경 변수 확인 ==='
            echo \"POSTGRES_USER: \${POSTGRES_USER:+설정됨}\"
            echo \"REDIS_PASSWORD: \${REDIS_PASSWORD:+설정됨}\"
            echo \"MONGO_USER: \${MONGO_USER:+설정됨}\"
            
            # 네트워크 생성
            echo '=== Docker 네트워크 확인/생성 ==='
            docker network create haptitalk_network 2>/dev/null || echo 'haptitalk_network 이미 존재'
            
            # 1단계: 핵심 데이터베이스 서비스 시작
            echo '=== 1단계: 핵심 데이터베이스 서비스 시작 ==='
            docker-compose -f docker-compose.prod.yml up -d postgres mongodb redis
            
            echo '=== 데이터베이스 서비스 준비 대기 (30초) ==='
            sleep 30
            
            # 데이터베이스 상태 확인
            echo '=== 데이터베이스 상태 확인 ==='
            docker-compose -f docker-compose.prod.yml ps postgres mongodb redis
            
            # 2단계: 메시징 시스템 시작
            echo '=== 2단계: 메시징 시스템 시작 ==='
            docker-compose -f docker-compose.prod.yml up -d zookeeper
            sleep 10
            docker-compose -f docker-compose.prod.yml up -d kafka
            sleep 15
            docker-compose -f docker-compose.prod.yml up -d kafka-ui
            
            # 3단계: API 게이트웨이 및 정적 웹 서버 시작
            echo '=== 3단계: API 게이트웨이 및 웹 서버 시작 ==='
            docker-compose -f docker-compose.prod.yml up -d kong static-web
            
            # 4단계: 모니터링 시스템 시작 (최소한의 포트 충돌 해결)
            echo '=== 4단계: 모니터링 시스템 시작 ==='
            
            # OTEL Collector 포트 충돌만 해결 (최소한으로)
            echo '=== OTEL Collector 포트 4318 충돌 해결 ==='
            
            # 포트 4318만 확인하고 정리 (다른 포트는 건드리지 않음)
            PIDS_4318=$(lsof -ti:4318 2>/dev/null || true)
            if [ -n "$PIDS_4318" ]; then
              echo "포트 4318 사용 프로세스 정리: $PIDS_4318"
              for pid in $PIDS_4318; do
                # 프로세스 이름 확인 후 OTEL/Jaeger 관련만 종료
                PROCESS_NAME=$(ps -p $pid -o comm= 2>/dev/null || echo "Unknown")
                if echo "$PROCESS_NAME" | grep -q "otel|jaeger|collector"; then
                  echo "OTEL/Jaeger 관련 프로세스 종료: PID=$pid, NAME=$PROCESS_NAME"
                  kill -TERM $pid 2>/dev/null || true
                else
                  echo "다른 프로세스는 유지: PID=$pid, NAME=$PROCESS_NAME"
                fi
              done
              sleep 2
            fi
            
            # 기존 OTEL/Jaeger 컨테이너만 정리 (다른 컨테이너는 건드리지 않음)
            docker-compose -f docker-compose.prod.yml stop otel-collector jaeger 2>/dev/null || true
            docker-compose -f docker-compose.prod.yml rm -f otel-collector jaeger 2>/dev/null || true
            
            # 원래 방식대로 모니터링 서비스 시작
            echo '모니터링 서비스 시작 중...'
            docker-compose -f docker-compose.prod.yml up -d elasticsearch
            sleep 20
            docker-compose -f docker-compose.prod.yml up -d kibana logstash filebeat
            sleep 15
            
            # Jaeger와 OTEL Collector를 순차적으로 시작 (더 짧은 대기시간)
            echo 'Jaeger 시작 중...'
            docker-compose -f docker-compose.prod.yml up -d jaeger
            sleep 10
            
            echo 'OTEL Collector 시작 중...'
            docker-compose -f docker-compose.prod.yml up -d otel-collector
            sleep 10
            
            # 나머지 모니터링 서비스 시작
            echo '=== 나머지 모니터링 서비스 시작 ==='
            docker-compose -f docker-compose.prod.yml up -d prometheus grafana node-exporter
            
            # 5단계: Exporters 시작
            echo '=== 5단계: Exporters 시작 ==='
            docker-compose -f docker-compose.prod.yml up -d mongodb-exporter postgres-exporter redis-exporter elasticsearch-exporter
            
            # 6단계: Blackbox Exporters 시작
            echo '=== 6단계: Blackbox Exporters 시작 ==='
            docker-compose -f docker-compose.prod.yml up -d kong-blackbox-exporter kibana-blackbox-exporter mongodb-blackbox-exporter
            
            # 중간 상태 확인
            echo '=== 중간 상태 확인 ==='
            docker-compose -f docker-compose.prod.yml ps
            
            # 7단계: 애플리케이션 서비스 시작
            echo '=== 7단계: 애플리케이션 서비스 시작 ==='
            
            # 항상 애플리케이션 서비스들을 시작하도록 수정 (이미지 체크 단순화)
            echo '애플리케이션 서비스들을 시작합니다.'
            
            # 개별적으로 시작 (실패 시에도 계속 진행)
            for service in auth-service session-service user-service feedback-service report-service realtime-service; do
              echo "서비스 시작: $service"
              docker-compose -f docker-compose.prod.yml up -d $service || echo "경고: $service 시작 실패"
              sleep 5
            done
            
            # 애플리케이션 서비스 시작 후 즉시 상태 확인 및 복구
            echo '=== 애플리케이션 서비스 상태 확인 및 자동 복구 ==='
            sleep 20  # 서비스 시작 대기
            
            # 실패한 애플리케이션 서비스 확인 (POSIX 호환)
            FAILED_APPS=\"\"
            FAILED_COUNT=0
            APP_SERVICES=\"auth-service session-service user-service feedback-service report-service realtime-service\"
            
            for service in \$APP_SERVICES; do
              if ! docker-compose -f docker-compose.prod.yml ps \$service | grep -q \"Up\"; then
                FAILED_APPS=\"\$FAILED_APPS \$service\"
                FAILED_COUNT=\$((FAILED_COUNT + 1))
                echo \"❌ \$service: 시작 실패\"
              else
                echo \"✅ \$service: 정상 시작\"
              fi
            done
            
            # 실패한 서비스가 있으면 즉시 복구 시도
            if [ \$FAILED_COUNT -gt 0 ]; then
              echo \"🚨 실패한 애플리케이션 서비스 \$FAILED_COUNT개 감지: \$FAILED_APPS\"
              echo \"즉시 자동 복구를 시도합니다...\"
              
              # GitHub Container Registry 인증 정보 설정
              export GHCR_PAT=\"\${{ secrets.GHCR_PAT }}\"
              export GITHUB_REPOSITORY_OWNER=\"\${{ github.repository_owner }}\"
              export IMAGE_TAG=\"\${{ needs.prepare_deployment.outputs.image_tag }}\"
              
              # 복구 스크립트 실행
              if [ -f \"infrastructure/automation/fix-app-services.sh\" ]; then
                echo \"📋 종합 복구 스크립트 실행 중...\"
                chmod +x infrastructure/automation/fix-app-services.sh
                ./infrastructure/automation/fix-app-services.sh || echo \"종합 복구 실패\"
              elif [ -f \"infrastructure/automation/quick-fix-app-services.sh\" ]; then
                echo \"📋 빠른 복구 스크립트 실행 중...\"
                chmod +x infrastructure/automation/quick-fix-app-services.sh
                ./infrastructure/automation/quick-fix-app-services.sh || echo \"빠른 복구 실패\"
              else
                echo \"⚠️ 복구 스크립트를 찾을 수 없습니다. 수동 복구를 시도합니다...\"
                
                # 수동 복구 시도
                for failed_service in \$FAILED_APPS; do
                  echo \"수동 복구 시도: \$failed_service\"
                  docker-compose -f docker-compose.prod.yml stop \$failed_service || true
                  docker-compose -f docker-compose.prod.yml rm -f \$failed_service || true
                  docker-compose -f docker-compose.prod.yml up -d \$failed_service || echo \"수동 복구 실패: \$failed_service\"
                  sleep 10
                done
              fi
              
              # 복구 후 재확인
              echo \"🔄 복구 후 애플리케이션 서비스 재확인...\"
              sleep 15
              
              RECOVERED_COUNT=0
              for service in \$APP_SERVICES; do
                if docker-compose -f docker-compose.prod.yml ps \$service | grep -q \"Up\"; then
                  RECOVERED_COUNT=\$((RECOVERED_COUNT + 1))
                  echo \"✅ \$service: 복구 성공\"
                else
                  echo \"❌ \$service: 복구 실패\"
                fi
              done
              
              echo \"복구 결과: \$RECOVERED_COUNT/6개 애플리케이션 서비스가 실행 중입니다.\"
              
              if [ \$RECOVERED_COUNT -ge 4 ]; then
                echo \"🎉 애플리케이션 서비스 복구 성공!\"
              else
                echo \"⚠️ 애플리케이션 서비스 복구가 완전하지 않습니다.\"
              fi
            else
              echo \"✅ 모든 애플리케이션 서비스가 정상적으로 시작되었습니다.\"
            fi
            
            # 최종 상태 확인
            echo '=== 최종 배포 상태 ==='
            docker-compose -f docker-compose.prod.yml ps
            
            # 실행 중인 컨테이너 수 확인
            RUNNING_CONTAINERS=$(docker-compose -f docker-compose.prod.yml ps | grep -c "Up")
            echo "실행 중인 컨테이너 수: $RUNNING_CONTAINERS"
            
            # 배포 후 정리 작업
            echo '=== 배포 후 정리 ==='
            docker system prune -f || echo '시스템 정리 완료'
            
            echo '=== 배포 완료 ==='
          "
      
      - name: Get Current Time
        id: current_time
        run: echo "time=$(date +'%Y-%m-%d %H:%M:%S')" >> $GITHUB_OUTPUT
      
      - name: Verify Deployment
        id: verify
        run: |
          # 배포 검증 (타임아웃 추가)
          VERIFICATION=$(timeout 180 ssh raspberry-pi '
            cd /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk
            
            echo "===== 디버깅 정보 시작 ====="
            echo "현재 디렉토리: $(pwd)"
            echo "현재 시간: $(date)"
            echo "사용자: $(whoami)"
            
            # 환경 변수 파일 확인
            echo "===== 환경 변수 파일 확인 ====="
            if [ -f ".env" ]; then
              echo ".env 파일 존재 - 크기: $(stat -c%s .env) bytes"
              echo "주요 환경 변수 확인:"
              grep -E "^(POSTGRES_|REDIS_|MONGO_)" .env | head -5 | sed "s/=.*$/=***/"
            else
              echo "❌ .env 파일이 없습니다!"
            fi
            
            # Docker 상태 확인
            echo "===== Docker 상태 확인 ====="
            echo "Docker 버전: $(docker --version)"
            echo "Docker Compose 버전: $(docker-compose --version)"
            echo "Docker 데몬 상태: $(docker info > /dev/null 2>&1 && echo "정상" || echo "오류")"
            
            # Docker Compose 파일 확인
            echo "===== Docker Compose 파일 확인 ====="
            if [ -f "docker-compose.prod.yml" ]; then
              echo "docker-compose.prod.yml 파일 존재 - 크기: $(stat -c%s docker-compose.prod.yml) bytes"
              echo "첫 번째 서비스:"
              head -20 docker-compose.prod.yml | grep -A 5 "services:"
            else
              echo "❌ docker-compose.prod.yml 파일이 없습니다!"
            fi
            
            # 환경 변수 로드 시도
            echo "===== 환경 변수 로드 시도 ====="
            if [ -f ".env" ]; then
              source .env
              echo "POSTGRES_USER 로드됨: ${POSTGRES_USER:+설정됨}"
              echo "REDIS_PASSWORD 로드됨: ${REDIS_PASSWORD:+설정됨}"
              echo "MONGO_USER 로드됨: ${MONGO_USER:+설정됨}"
            fi
            
            # Docker 네트워크 확인
            echo "===== Docker 네트워크 확인 ====="
            docker network ls | grep haptitalk || echo "haptitalk 네트워크 없음"
            
            # 실행 중인 컨테이너 확인
            echo "===== 실행 중인 컨테이너 ====="
            docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
            
            # 모든 컨테이너 확인 (중지된 것 포함)
            echo "===== 모든 컨테이너 (중지된 것 포함) ====="
            docker ps -a --format "table {{.Names}}\t{{.Status}}\t{{.Image}}" | head -20
            
            # Docker Compose 서비스 상태
            echo "===== Docker Compose 서비스 상태 ====="
            if [ -f "docker-compose.prod.yml" ]; then
              timeout 30 docker-compose -f docker-compose.prod.yml ps || echo "docker-compose ps 타임아웃"
            else
              echo "docker-compose.prod.yml 파일 없음"
            fi
            
            # 디스크 공간 확인
            echo "===== 시스템 리소스 ====="
            echo "디스크 사용률:"
            df -h / | grep -v "Filesystem"
            echo "메모리 사용률:"
            free -h | grep -E "Mem:|Swap:"
            
            # 간단한 헬스체크 (docker-compose 기반으로 수정)
            echo "===== 간단한 헬스체크 ====="
            
            # Docker Compose 서비스 상태 직접 확인
            COMPOSE_PS_OUTPUT=$(timeout 30 docker-compose -f docker-compose.prod.yml ps 2>/dev/null || echo "TIMEOUT")
            echo "Docker Compose 서비스 상태:"
            echo "$COMPOSE_PS_OUTPUT"
            
            # PostgreSQL 확인
            if echo "$COMPOSE_PS_OUTPUT" | grep -q "postgres.*Up"; then
              echo "PostgreSQL 컨테이너: 실행 중"
              POSTGRES_HEALTH=$(docker-compose -f docker-compose.prod.yml exec -T postgres pg_isready 2>/dev/null && echo "OK" || echo "FAIL")
              echo "PostgreSQL 헬스체크: $POSTGRES_HEALTH"
            else
              echo "PostgreSQL 컨테이너: 실행 중이 아님"
              echo "PostgreSQL: FAIL"
            fi
            
            # Redis 확인
            if echo "$COMPOSE_PS_OUTPUT" | grep -q "redis.*Up"; then
              echo "Redis 컨테이너: 실행 중"
              REDIS_HEALTH=$(docker-compose -f docker-compose.prod.yml exec -T redis redis-cli ping 2>/dev/null && echo "OK" || echo "FAIL")
              echo "Redis 헬스체크: $REDIS_HEALTH"
            else
              echo "Redis 컨테이너: 실행 중이 아님"
              echo "Redis: FAIL"
            fi
            
            # Kong 확인
            if echo "$COMPOSE_PS_OUTPUT" | grep -q "kong.*Up"; then
              echo "Kong 컨테이너: 실행 중"
              KONG_HEALTH=$(docker-compose -f docker-compose.prod.yml exec -T kong kong version 2>/dev/null && echo "OK" || echo "FAIL")
              echo "Kong 헬스체크: $KONG_HEALTH"
            else
              echo "Kong 컨테이너: 실행 중이 아님"
              echo "Kong: FAIL"
            fi
            
            # MongoDB 확인
            if echo "$COMPOSE_PS_OUTPUT" | grep -q "mongodb.*Up"; then
              echo "MongoDB 컨테이너: 실행 중"
              MONGODB_HEALTH=$(docker-compose -f docker-compose.prod.yml exec -T mongodb mongosh --quiet --eval "db.runCommand({ping: 1}).ok" 2>/dev/null | grep -q "1" && echo "OK" || echo "FAIL")
              echo "MongoDB 헬스체크: $MONGODB_HEALTH"
            else
              echo "MongoDB 컨테이너: 실행 중이 아님"
              echo "MongoDB: FAIL"
            fi
            
            # Kafka 확인
            if echo "$COMPOSE_PS_OUTPUT" | grep -q "kafka.*Up"; then
              echo "Kafka 컨테이너: 실행 중"
              KAFKA_HEALTH=$(docker-compose -f docker-compose.prod.yml exec -T kafka kafka-topics.sh --bootstrap-server localhost:9092 --list 2>/dev/null >/dev/null && echo "OK" || echo "FAIL")
              echo "Kafka 헬스체크: $KAFKA_HEALTH"
            else
              echo "Kafka 컨테이너: 실행 중이 아님"
              echo "Kafka: FAIL"
            fi
            
            # 애플리케이션 서비스 상태 확인
            echo "===== 애플리케이션 서비스 상태 ====="
            for service in auth-service session-service user-service feedback-service report-service realtime-service; do
              if echo "$COMPOSE_PS_OUTPUT" | grep -q "$service.*Up"; then
                # 헬스체크 상태도 확인
                if echo "$COMPOSE_PS_OUTPUT" | grep "$service" | grep -q "healthy"; then
                  echo "$service: OK (healthy)"
                else
                  echo "$service: OK (running)"
                fi
              else
                echo "$service: FAIL"
              fi
            done
            
            echo "===== 디버깅 정보 끝 ====="
          ')
          
          echo "$VERIFICATION"
          
          # 핵심 서비스 실행 여부 확인 (docker-compose 기반으로 수정)
          CORE_SERVICES_RUNNING=0
          
          # 각 핵심 서비스가 실행 중인지 확인
          if echo "$VERIFICATION" | grep -q "PostgreSQL 컨테이너: 실행 중"; then
            CORE_SERVICES_RUNNING=$((CORE_SERVICES_RUNNING + 1))
          fi
          
          if echo "$VERIFICATION" | grep -q "Redis 컨테이너: 실행 중"; then
            CORE_SERVICES_RUNNING=$((CORE_SERVICES_RUNNING + 1))
          fi
          
          if echo "$VERIFICATION" | grep -q "MongoDB 컨테이너: 실행 중"; then
            CORE_SERVICES_RUNNING=$((CORE_SERVICES_RUNNING + 1))
          fi
          
          if echo "$VERIFICATION" | grep -q "Kafka 컨테이너: 실행 중"; then
            CORE_SERVICES_RUNNING=$((CORE_SERVICES_RUNNING + 1))
          fi
          
          # Kong은 unhealthy 상태여도 실행 중이면 OK로 간주
          if echo "$VERIFICATION" | grep -q "Kong 컨테이너: 실행 중"; then
            CORE_SERVICES_RUNNING=$((CORE_SERVICES_RUNNING + 1))
          fi
          
          # 애플리케이션 서비스 실행 여부 확인 (OK 패턴으로 수정)
          APP_SERVICES_RUNNING=$(echo "$VERIFICATION" | grep -c ": OK (healthy)\|: OK (running)")
          
          echo "핵심 서비스 실행 중: $CORE_SERVICES_RUNNING/5"
          echo "애플리케이션 서비스 실행 중: $APP_SERVICES_RUNNING"
          
          # 성공 조건: 핵심 서비스 4개 이상 + 애플리케이션 서비스 3개 이상
          if [ "$CORE_SERVICES_RUNNING" -ge 4 ] && [ "$APP_SERVICES_RUNNING" -ge 3 ]; then
            echo "deploy_success=true" >> $GITHUB_OUTPUT
            echo "verification_result=배포 성공! 핵심 서비스 $CORE_SERVICES_RUNNING/5개, 앱 서비스 $APP_SERVICES_RUNNING개 실행 중" >> $GITHUB_OUTPUT
          else
            echo "deploy_success=false" >> $GITHUB_OUTPUT
            echo "verification_result=배포 실패. 핵심 서비스 $CORE_SERVICES_RUNNING/5개, 앱 서비스 $APP_SERVICES_RUNNING개만 실행 중" >> $GITHUB_OUTPUT
          fi
      
      - name: Fix Container Issues
        if: steps.verify.outputs.deploy_success == 'false'
        id: fix_containers
        run: |
          echo "🔧 컨테이너 문제 자동 해결 시작..."
          
          CONTAINER_FIX_RESULT=$(ssh raspberry-pi '
            cd /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk
            
            echo "===== 컨테이너 문제 진단 및 해결 시작: $(date) ====="
            
            # 1. 문제가 있는 컨테이너 식별
            echo "📊 문제 컨테이너 식별 중..."
            PROBLEMATIC_CONTAINERS=$(docker ps -a --filter "status=restarting" --filter "status=exited" --format "{{.Names}}" | grep haptitalk || true)
            
            if [[ -n "$PROBLEMATIC_CONTAINERS" ]]; then
              echo "🚨 문제가 있는 컨테이너들:"
              echo "$PROBLEMATIC_CONTAINERS"
              
              # 특별히 Filebeat와 Kong 문제 확인
              if echo "$PROBLEMATIC_CONTAINERS" | grep -q "filebeat"; then
                echo "🔍 Filebeat 문제 감지 - 외부 설정 파일 방식으로 수정 중..."
                docker-compose -f docker-compose.prod.yml stop filebeat || true
                docker-compose -f docker-compose.prod.yml up -d --no-deps filebeat
              fi
              
              if echo "$PROBLEMATIC_CONTAINERS" | grep -q "kong"; then
                echo "🔍 Kong 문제 감지 - healthcheck 수정 및 재시작 중..."
                docker-compose -f docker-compose.prod.yml stop kong || true
                docker-compose -f docker-compose.prod.yml up -d --no-deps kong
              fi
              
              # OTEL Collector 및 Jaeger 간단 재시작 (문제 있을 때만)
              if echo "$PROBLEMATIC_CONTAINERS" | grep -q "otel-collector\|jaeger"; then
                echo "🔍 OTEL/Jaeger 문제 감지 - 간단 재시작"
                docker-compose -f docker-compose.prod.yml restart otel-collector jaeger || true
                sleep 10
              fi
            else
              echo "✅ 재시작 중인 컨테이너 없음"
            fi
            
            # 2. 시스템 리소스 확인 및 최적화
            echo "💾 시스템 리소스 확인 중..."
            MEMORY_USAGE=$(free | grep Mem | awk "{printf \"%.1f\", \$3/\$2 * 100.0}")
            SWAP_USAGE=$(free | grep Swap | awk "{if(\$2>0) printf \"%.1f\", \$3/\$2 * 100.0; else print \"0\"}")
            
            echo "메모리 사용률: ${MEMORY_USAGE}%"
            echo "Swap 사용률: ${SWAP_USAGE}%"
            
            # Swap이 90% 이상 사용 중이면 확장
            if (( $(echo "$SWAP_USAGE > 90" | bc -l) )); then
              echo "⚠️ Swap 메모리 부족 - 확장 중..."
              sudo swapoff /var/swap || true
              sudo dd if=/dev/zero of=/var/swap bs=1M count=2048 2>/dev/null
              sudo mkswap /var/swap
              sudo swapon /var/swap
              echo "✅ Swap 메모리가 2GB로 확장되었습니다."
            fi
            
            # 3. 중복 컨테이너 제거
            echo "🗑️ 중복 컨테이너 제거 중..."
            docker ps -a --format "{{.Names}}" | grep -E "^[a-z]+_[a-z]+$" | head -5 | xargs -r docker rm -f || true
            
            # 4. Docker 시스템 정리 (볼륨 보존)
            echo "🧹 Docker 시스템 정리 중..."
            docker system prune -f || true
            
            # 5. 메모리 제한이 있는 서비스들 재시작
            echo "🚀 메모리 제한 적용된 서비스들 재시작 중..."
            docker-compose -f docker-compose.prod.yml up -d --no-deps kong elasticsearch logstash filebeat
            
            # 6. 잠시 대기 후 상태 확인
            echo "⏳ 서비스 안정화 대기 중..."
            sleep 30
            
            # 7. 수정 후 상태 확인
            echo "📊 수정 후 컨테이너 상태:"
            docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
            
            # 8. 핵심 서비스 헬스체크
            echo "🏥 핵심 서비스 헬스체크..."
            
            # Kong 상태 확인
            KONG_STATUS="FAIL"
            if curl -f http://localhost:8001/status >/dev/null 2>&1; then
              KONG_STATUS="OK"
            fi
            echo "Kong: $KONG_STATUS"
            
            # PostgreSQL 상태 확인
            POSTGRES_STATUS="FAIL"
            if docker-compose -f docker-compose.prod.yml exec -T postgres pg_isready -U "$POSTGRES_USER" -d "$POSTGRES_DB" >/dev/null 2>&1; then
              POSTGRES_STATUS="OK"
            fi
            echo "PostgreSQL: $POSTGRES_STATUS"
            
            # Redis 상태 확인
            REDIS_STATUS="FAIL"
            if docker-compose -f docker-compose.prod.yml exec -T redis redis-cli -a "$REDIS_PASSWORD" ping >/dev/null 2>&1; then
              REDIS_STATUS="OK"
            fi
            echo "Redis: $REDIS_STATUS"
            
            # OTEL Collector 상태 확인
            OTEL_STATUS="FAIL"
            if curl -f http://localhost:55679/debug/servicez >/dev/null 2>&1; then
              OTEL_STATUS="OK"
            fi
            echo "OTEL Collector: $OTEL_STATUS"
            
            # Jaeger 상태 확인
            JAEGER_STATUS="FAIL"
            if curl -f http://localhost:16686 >/dev/null 2>&1; then
              JAEGER_STATUS="OK"
            fi
            echo "Jaeger: $JAEGER_STATUS"
            
            # 애플리케이션 서비스 실패 확인 및 자동 복구
            echo "🔍 애플리케이션 서비스 상태 확인 및 복구..."
            
            # 실패한 애플리케이션 서비스 수 계산
            FAILED_APP_SERVICES=0
            for app_service in auth-service session-service user-service feedback-service report-service realtime-service; do
              if ! docker-compose -f docker-compose.prod.yml ps $app_service | grep -q "Up"; then
                ((FAILED_APP_SERVICES++))
                echo "❌ $app_service: 실행 중이 아님"
              else
                echo "✅ $app_service: 정상 실행 중"
              fi
            done
            
            # 실패한 애플리케이션 서비스가 3개 이상인 경우 자동 복구 스크립트 실행
            if [[ $FAILED_APP_SERVICES -ge 3 ]]; then
              echo "🚨 애플리케이션 서비스 $FAILED_APP_SERVICES개 실패 감지 - 자동 복구 시작"
              
              # fix-app-services.sh 스크립트 실행
              if [ -f "infrastructure/automation/fix-app-services.sh" ]; then
                echo "📋 자동 복구 스크립트 실행 중..."
                chmod +x infrastructure/automation/fix-app-services.sh
                
                # GitHub Container Registry 인증 정보 추가
                export GHCR_PAT="${{ secrets.GHCR_PAT }}"
                export GITHUB_REPOSITORY_OWNER="${{ github.repository_owner }}"
                export IMAGE_TAG="${{ needs.prepare_deployment.outputs.image_tag }}"
                
                # 복구 스크립트 실행
                if ./infrastructure/automation/fix-app-services.sh; then
                  echo "✅ 애플리케이션 서비스 자동 복구 성공"
                  
                  # 복구 후 재확인
                  echo "🔄 복구 후 애플리케이션 서비스 재확인..."
                  sleep 15
                  
                  RECOVERED_SERVICES=0
                  for app_service in auth-service session-service user-service feedback-service report-service realtime-service; do
                    if docker-compose -f docker-compose.prod.yml ps $app_service | grep -q "Up"; then
                      ((RECOVERED_SERVICES++))
                      echo "✅ $app_service: 복구됨"
                    else
                      echo "❌ $app_service: 여전히 실패"
                    fi
                  done
                  
                  echo "복구된 애플리케이션 서비스: $RECOVERED_SERVICES/6개"
                  
                  if [[ $RECOVERED_SERVICES -ge 4 ]]; then
                    echo "🎉 애플리케이션 서비스 복구 성공 ($RECOVERED_SERVICES/6개)"
                  else
                    echo "⚠️ 애플리케이션 서비스 일부만 복구됨 ($RECOVERED_SERVICES/6개)"
                  fi
                else
                  echo "❌ 애플리케이션 서비스 자동 복구 실패"
                fi
              else
                echo "⚠️ 애플리케이션 복구 스크립트를 찾을 수 없습니다: infrastructure/automation/fix-app-services.sh"
                
                # 빠른 복구 스크립트 시도
                if [ -f "infrastructure/automation/quick-fix-app-services.sh" ]; then
                  echo "📋 빠른 복구 스크립트로 시도..."
                  chmod +x infrastructure/automation/quick-fix-app-services.sh
                  ./infrastructure/automation/quick-fix-app-services.sh || echo "빠른 복구 실패"
                fi
              fi
            elif [[ $FAILED_APP_SERVICES -gt 0 ]]; then
              echo "⚠️ 애플리케이션 서비스 $FAILED_APP_SERVICES개 실패 - 빠른 복구 시도"
              
              # 일부 서비스만 실패한 경우 빠른 복구 스크립트 사용
              if [ -f "infrastructure/automation/quick-fix-app-services.sh" ]; then
                chmod +x infrastructure/automation/quick-fix-app-services.sh
                echo "📋 빠른 복구 스크립트 실행 중..."
                ./infrastructure/automation/quick-fix-app-services.sh || echo "빠른 복구 실패"
              fi
            else
              echo "✅ 모든 애플리케이션 서비스가 정상입니다"
            fi
            
            # 수정 성공 여부 판단 (핵심 서비스 + 모니터링 서비스)
            CORE_OK=0
            MONITORING_OK=0
            
            # 핵심 서비스 카운트
            [[ "$KONG_STATUS" == "OK" ]] && ((CORE_OK++))
            [[ "$POSTGRES_STATUS" == "OK" ]] && ((CORE_OK++))
            [[ "$REDIS_STATUS" == "OK" ]] && ((CORE_OK++))
            
            # 모니터링 서비스 카운트 (선택적)
            [[ "$OTEL_STATUS" == "OK" ]] && ((MONITORING_OK++))
            [[ "$JAEGER_STATUS" == "OK" ]] && ((MONITORING_OK++))
            
            # 성공 조건: 핵심 서비스 3개 모두 OK
            if [[ $CORE_OK -eq 3 ]]; then
              echo "CONTAINER_FIX_SUCCESS=true"
              echo "✅ 컨테이너 문제 해결 성공 (핵심: $CORE_OK/3, 모니터링: $MONITORING_OK/2)"
            else
              echo "CONTAINER_FIX_SUCCESS=false"
              echo "❌ 핵심 서비스 중 일부가 여전히 문제 상태 (핵심: $CORE_OK/3, 모니터링: $MONITORING_OK/2)"
            fi
            
            echo "===== 컨테이너 문제 해결 완료: $(date) ====="
          ')
          
          echo "$CONTAINER_FIX_RESULT"
          
          # 수정 결과 확인
          if echo "$CONTAINER_FIX_RESULT" | grep -q "CONTAINER_FIX_SUCCESS=true"; then
            echo "✅ 컨테이너 문제가 성공적으로 해결되었습니다."
            echo "container_fix_success=true" >> $GITHUB_OUTPUT
            
            # 문제 해결 후 재검증
            echo "🔄 문제 해결 후 재검증 중..."
            sleep 10
            
            # 재검증 실행
            REVERIFY_RESULT=$(ssh raspberry-pi '
              cd /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk
              
              echo "===== 재검증 시작 ====="
              
              # 핵심 서비스 재확인
              SERVICES_OK=0
              TOTAL_SERVICES=6
              
              # Kong 확인
              if curl -f http://localhost:8001/status >/dev/null 2>&1; then
                echo "✅ Kong API Gateway: 정상"
                ((SERVICES_OK++))
              else
                echo "❌ Kong API Gateway: 오류"
              fi
              
              # PostgreSQL 확인
              if docker-compose -f docker-compose.prod.yml exec -T postgres pg_isready -U "$POSTGRES_USER" -d "$POSTGRES_DB" >/dev/null 2>&1; then
                echo "✅ PostgreSQL: 정상"
                ((SERVICES_OK++))
              else
                echo "❌ PostgreSQL: 오류"
              fi
              
              # Redis 확인
              if docker-compose -f docker-compose.prod.yml exec -T redis redis-cli -a "$REDIS_PASSWORD" ping >/dev/null 2>&1; then
                echo "✅ Redis: 정상"
                ((SERVICES_OK++))
              else
                echo "❌ Redis: 오류"
              fi
              
              # MongoDB 확인
              if docker-compose -f docker-compose.prod.yml exec -T mongodb mongosh --quiet --eval "db.runCommand({ping: 1}).ok" 2>/dev/null | grep -q "1"; then
                echo "✅ MongoDB: 정상"
                ((SERVICES_OK++))
              else
                echo "❌ MongoDB: 오류"
              fi
              
              # Kafka 확인
              if docker-compose -f docker-compose.prod.yml exec -T kafka kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1; then
                echo "✅ Kafka: 정상"
                ((SERVICES_OK++))
              else
                echo "❌ Kafka: 오류"
              fi
              
              # Elasticsearch 확인
              if curl -f http://localhost:9200/_cluster/health >/dev/null 2>&1; then
                echo "✅ Elasticsearch: 정상"
                ((SERVICES_OK++))
              else
                echo "❌ Elasticsearch: 오류"
              fi
              
              # OTEL Collector 확인 (추가)
              if curl -f http://localhost:55679/debug/servicez >/dev/null 2>&1; then
                echo "✅ OTEL Collector: 정상"
                ((SERVICES_OK++))
              else
                echo "❌ OTEL Collector: 오류"
              fi
              
              # Jaeger 확인 (추가)
              if curl -f http://localhost:16686 >/dev/null 2>&1; then
                echo "✅ Jaeger: 정상"
                ((SERVICES_OK++))
              else
                echo "❌ Jaeger: 오류"
              fi
              
              TOTAL_SERVICES=8  # 서비스 총 개수 업데이트
              echo "정상 서비스: $SERVICES_OK/$TOTAL_SERVICES"
              
              # 성공 조건: 8개 중 6개 이상 (75% 이상)
              if [[ $SERVICES_OK -ge 6 ]]; then
                echo "REVERIFY_SUCCESS=true"
              else
                echo "REVERIFY_SUCCESS=false"
              fi
            ')
            
            echo "$REVERIFY_RESULT"
            
            if echo "$REVERIFY_RESULT" | grep -q "REVERIFY_SUCCESS=true"; then
              echo "✅ 재검증 성공 - 배포가 정상적으로 복구되었습니다."
              echo "reverify_success=true" >> $GITHUB_OUTPUT
            else
              echo "❌ 재검증 실패 - 여전히 문제가 있습니다."
              echo "reverify_success=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "❌ 컨테이너 문제 해결에 실패했습니다."
            echo "container_fix_success=false" >> $GITHUB_OUTPUT
            echo "reverify_success=false" >> $GITHUB_OUTPUT
          fi

      - name: Rollback on Failure
        if: steps.verify.outputs.deploy_success == 'false'
        id: final_rollback
        run: |
          echo "배포 검증 실패, 롤백을 진행합니다..."
          
          ROLLBACK_RESULT=$(ssh raspberry-pi '
            cd /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk
            
            # 환경 변수 로드
            source .env
            
            echo "===== 롤백 시작 ====="
            echo "롤백 시작 시간: $(date)"
            
            # 현재 상태 백업
            echo "현재 컨테이너 상태 백업..."
            docker-compose -f docker-compose.prod.yml ps > rollback_before_state.log
            
            # 모든 서비스 정지 (graceful shutdown)
            echo "서비스 정지 중..."
            docker-compose -f docker-compose.prod.yml stop
            
            # 컨테이너 제거 (orphan 컨테이너 포함)
            echo "컨테이너 정리 중..."
            docker-compose -f docker-compose.prod.yml down --remove-orphans --volumes
            
            # 최신 이미지 pull (롤백을 위해)
            echo "최신 이미지 확인 중..."
            docker-compose -f docker-compose.prod.yml pull --ignore-pull-failures
            
            # 이전 환경 설정 복원 (있는 경우)
            if [ -f ".env.backup" ]; then
              echo "이전 환경 설정 복원 중..."
              cp .env.backup .env
            fi
            
            # 핵심 인프라 서비스부터 단계적으로 시작
            echo "핵심 인프라 서비스 시작 중..."
            docker-compose -f docker-compose.prod.yml up -d postgres mongodb redis zookeeper
            sleep 10
            
            echo "메시징 및 API 게이트웨이 시작 중..."
            docker-compose -f docker-compose.prod.yml up -d kafka kong
            sleep 10
            
            echo "모니터링 서비스 시작 중..."
            docker-compose -f docker-compose.prod.yml up -d prometheus grafana elasticsearch kibana
            sleep 15
            
            # 롤백 후 상태 확인
            echo "===== 롤백 후 상태 확인 ====="
            docker-compose -f docker-compose.prod.yml ps
            
            # 기본 헬스체크
            echo "===== 롤백 후 헬스체크 ====="
            
            # PostgreSQL 체크
            POSTGRES_CHECK=$(docker-compose -f docker-compose.prod.yml exec -T postgres pg_isready -U "$POSTGRES_USER" -d "$POSTGRES_DB" 2>/dev/null && echo "OK" || echo "FAIL")
            echo "PostgreSQL: $POSTGRES_CHECK"
            
            # Redis 체크
            REDIS_CHECK=$(docker-compose -f docker-compose.prod.yml exec -T redis redis-cli -a "$REDIS_PASSWORD" ping 2>/dev/null && echo "OK" || echo "FAIL")
            echo "Redis: $REDIS_CHECK"
            
            # MongoDB 체크
            MONGODB_CHECK=$(docker-compose -f docker-compose.prod.yml exec -T mongodb mongosh --quiet --eval "db.runCommand({ping: 1}).ok" 2>/dev/null | grep -q "1" && echo "OK" || echo "FAIL")
            echo "MongoDB: $MONGODB_CHECK"
            
            # 롤백 완료 시간
            echo "===== 롤백 완료: $(date) ====="
            
            # 롤백 성공 여부 판단
            if [[ "$POSTGRES_CHECK" == "OK" && "$REDIS_CHECK" == "OK" && "$MONGODB_CHECK" == "OK" ]]; then
              echo "ROLLBACK_SUCCESS=true"
            else
              echo "ROLLBACK_SUCCESS=false"
            fi
          ')
          
          echo "$ROLLBACK_RESULT"
          
          # 롤백 결과 확인
          if echo "$ROLLBACK_RESULT" | grep -q "ROLLBACK_SUCCESS=true"; then
            echo "✅ 롤백이 성공적으로 완료되었습니다."
            echo "rollback_success=true" >> $GITHUB_OUTPUT
          else
            echo "❌ 롤백 중 문제가 발생했습니다. 수동 개입이 필요합니다."
            echo "rollback_success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Send Deployment Notification
        if: always()
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL || 'deployments' }}
          SLACK_COLOR: ${{ (steps.verify.outputs.deploy_success == 'false' && 'danger') || job.status }}
          SLACK_TITLE: Raspberry Pi Deployment - ${{ needs.prepare_deployment.outputs.environment }}
          SLACK_MESSAGE: |
            *🚀 라즈베리파이 ARM64 배포 결과*
            
            *환경:* ${{ needs.prepare_deployment.outputs.environment }}
            *상태:* ${{ steps.verify.outputs.deploy_success == 'false' && (steps.fix_containers.outputs.reverify_success == 'true' && '✅ 문제 해결 성공' || (steps.final_rollback.outputs.rollback_success == 'true' && '⚠️ 배포 실패 (롤백 완료)' || '🚨 배포 및 롤백 실패')) || '✅ 배포 성공' }}
            *플랫폼:* ${{ needs.prepare_deployment.outputs.platform_arch }}
            *Kong 이미지:* ${{ needs.prepare_deployment.outputs.kong_image }}
            
            *배포 서비스:* ${{ needs.prepare_deployment.outputs.specific_services || '전체 서비스' }}
            *강제 재시작:* ${{ needs.prepare_deployment.outputs.force_restart }}
            *이미지 상태:* ${{ needs.validate_images.outputs.images_available == 'true' && '✅ 모든 이미지 사용 가능' || '⚠️ 일부 이미지 누락' }}
            ${{ needs.validate_images.outputs.missing_images && format('*누락된 이미지:* {0}', needs.validate_images.outputs.missing_images) || '' }}
            
            *검증 결과:* ${{ steps.verify.outputs.verification_result || '검증 정보 없음' }}
            ${{ steps.verify.outputs.deploy_success == 'false' && (steps.fix_containers.outputs.reverify_success == 'true' && '*문제 해결:* ✅ 컨테이너 문제 자동 해결 성공' || format('*롤백 상태:* {0}', (steps.final_rollback.outputs.rollback_success == 'true' && '✅ 롤백 성공' || '❌ 롤백 실패 - 수동 개입 필요'))) || '' }}
            
            *시스템 리소스:*
            • 디스크 공간: ${{ steps.disk_check.outputs.disk_usage }}% 사용 (가용: ${{ steps.disk_check.outputs.available_space }})
            • 메모리: 가용 ${{ steps.ram_check.outputs.available_ram }}
            
            *배포 정보:*
            • 브랜치: ${{ github.ref_name }}
            • 커밋: ${{ github.sha }}
            • 배포 시간: ${{ steps.current_time.outputs.time }}
            • 워크플로우: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|상세 보기>
            
            ${{ (steps.verify.outputs.deploy_success == 'true' || steps.fix_containers.outputs.reverify_success == 'true') && '*📊 모니터링 대시보드:*
            • Grafana: http://${{ secrets.RASPBERRY_PI_IP }}:3333
            • Prometheus: http://${{ secrets.RASPBERRY_PI_IP }}:9090
            • Kibana: http://${{ secrets.RASPBERRY_PI_IP }}:5601
            • Jaeger: http://${{ secrets.RASPBERRY_PI_IP }}:16686
            • Elasticsearch: http://${{ secrets.RASPBERRY_PI_IP }}:9200
            
            *🔧 시스템 메트릭:*
            • Node Exporter: http://${{ secrets.RASPBERRY_PI_IP }}:9100/metrics
            • MongoDB Exporter: http://${{ secrets.RASPBERRY_PI_IP }}:9216/metrics
            • PostgreSQL Exporter: http://${{ secrets.RASPBERRY_PI_IP }}:9187/metrics
            • Redis Exporter: http://${{ secrets.RASPBERRY_PI_IP }}:9121/metrics
            • Elasticsearch Exporter: http://${{ secrets.RASPBERRY_PI_IP }}:9114/metrics
            • Logstash API: http://${{ secrets.RASPBERRY_PI_IP }}:9600' || '' }}
            
            ${{ steps.verify.outputs.deploy_success == 'false' && steps.fix_containers.outputs.reverify_success != 'true' && '*⚠️ 문제 해결이 필요합니다:*
            • 라즈베리파이 SSH 접속: `ssh ${{ secrets.RASPBERRY_PI_USER }}@${{ secrets.RASPBERRY_PI_IP }}`
            • 로그 확인: `cd /home/${{ secrets.RASPBERRY_PI_USER }}/haptitalk && docker-compose -f docker-compose.prod.yml logs --tail=100`
            • 수동 재시작: `docker-compose -f docker-compose.prod.yml restart`
            • 문제 해결 스크립트: `chmod +x infrastructure/deployment/fix-raspberry-pi-containers.sh && ./infrastructure/deployment/fix-raspberry-pi-containers.sh`' || '' }}
          SLACK_FOOTER: 'HaptiTalk CI/CD - ARM64 Deployment Pipeline'
        continue-on-error: true
